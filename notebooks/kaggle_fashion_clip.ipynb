{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx9pUccqlBh-"
      },
      "source": [
        "# Todo\n",
        "- [x] load clip\n",
        "- [x] load images\n",
        "- [x] batch run inference on images\n",
        "- [x] figure out how to load images from zip\n",
        "- [x] save embeddings\n",
        "- [x] benchmark, gpu: 2h\n",
        "- [x] upload to gcp bucket\n",
        "- [x] embed text as well\n",
        "- [ ] predict a baseline score for kaggle, on image+description cos similarity only"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfH2Q0BIkFry",
        "outputId": "adc381f3-9361-49ab-dd92-cb7140cabc1b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr 20 08:20:00 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-ANzqxPrdZI"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK9UnBnZmmmZ"
      },
      "source": [
        "Don't forget to upload your **kaggle.json** for authentication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8gReixDle8S"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfKoG--brGPC"
      },
      "source": [
        "1. get data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yxJAauDm86J",
        "outputId": "4c0c79b2-9034-4b6b-b4dd-a3fd708765fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/kaggle.json'\n",
            "Downloading h-and-m-personalized-fashion-recommendations.zip to /content\n",
            "100% 28.7G/28.7G [02:35<00:00, 199MB/s]\n",
            "100% 28.7G/28.7G [02:35<00:00, 198MB/s]\n"
          ]
        }
      ],
      "source": [
        "!KAGGLE_CONFIG_DIR=/content kaggle competitions download -c h-and-m-personalized-fashion-recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clse0OTT0mpo"
      },
      "source": [
        "2. mount zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HVEaOacCyXUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bc60118-53e6-4a4a-9cdb-9d63be628f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libzip4\n",
            "The following NEW packages will be installed:\n",
            "  fuse-zip libzip4\n",
            "0 upgraded, 2 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 65.6 kB of archives.\n",
            "After this operation, 178 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libzip4 amd64 1.1.2-1.1 [37.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fuse-zip amd64 0.4.4-1 [27.9 kB]\n",
            "Fetched 65.6 kB in 1s (92.1 kB/s)\n",
            "Selecting previously unselected package libzip4:amd64.\n",
            "(Reading database ... 155455 files and directories currently installed.)\n",
            "Preparing to unpack .../libzip4_1.1.2-1.1_amd64.deb ...\n",
            "Unpacking libzip4:amd64 (1.1.2-1.1) ...\n",
            "Selecting previously unselected package fuse-zip.\n",
            "Preparing to unpack .../fuse-zip_0.4.4-1_amd64.deb ...\n",
            "Unpacking fuse-zip (0.4.4-1) ...\n",
            "Setting up libzip4:amd64 (1.1.2-1.1) ...\n",
            "Setting up fuse-zip (0.4.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y fuse-zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "C719KyYAxXcE"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/archive\n",
        "!fuse-zip /content/h-and-m-personalized-fashion-recommendations.zip /content/archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_njuz7q4zhr4"
      },
      "outputs": [],
      "source": [
        "# to unmount\n",
        "# !fusermount -u /content/archive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDFoZR1MrIt_"
      },
      "source": [
        "3. get clip model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jg9N13AEq8JR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e341b7c0-a8af-49fe-f698-97ef2506aa64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ur52s49x\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-ur52s49x\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.11.1+cu111)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 8.8 MB/s \n",
            "\u001b[?25hCollecting https://github.com/pyg-team/pytorch_geometric/archive/master.zip\n",
            "  Downloading https://github.com/pyg-team/pytorch_geometric/archive/master.zip\n",
            "\u001b[K     / 2.4 MB 3.8 MB/s\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (1.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (3.0.8)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric==2.0.5) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==2.0.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==2.0.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==2.0.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==2.0.5) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric==2.0.5) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric==2.0.5) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.5-py3-none-any.whl size=628231 sha256=e80dc7c1879159734c96d6a6dce3ad8e1b8a2693f16dd8fee9cac36fed08320f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4huw9gnf/wheels/70/53/71/38e50390ffab43b7bf5e55f1cdec398bbb09b9b3d2facb4478\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "# !pip install torch-scatter -f https://data.pyg.org/whl/torch-1.11.0+${CUDA}.html\n",
        "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-1.11.0+${CUDA}.html\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "# !pip install torch-scatter torch-sparse\n",
        "!pip install https://github.com/pyg-team/pytorch_geometric/archive/master.zip "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LliSeZa7yto5"
      },
      "source": [
        "# Get embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3X3L0ZkrUn2"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import clip\n",
        "\n",
        "device = \"cuda\" if t.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo41qRdisFPK"
      },
      "outputs": [],
      "source": [
        "# TODO to use tpu\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# device = xm.xla_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhJdGWOip0W_"
      },
      "outputs": [],
      "source": [
        "# example useage of clip ala. https://github.com/openai/CLIP\n",
        "# model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "# image = preprocess(Image.open(\"/content/images/010/0108775015.jpg\")).unsqueeze(0).to(device)\n",
        "# text = clip.tokenize([\"a dress\", \"a dog\", \"a cat\"]).to(device)\n",
        "\n",
        "# with t.no_grad():\n",
        "#     image_features = model.encode_image(image)\n",
        "#     text_features = model.encode_text(text)\n",
        "    \n",
        "#     logits_per_image, logits_per_text = model(image, text)\n",
        "#     probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "# print(\"Label probs:\", probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEDpSHBc5W71"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from t.utils.data import Dataset, DataLoader\n",
        "from os.path import exists\n",
        "\n",
        "\n",
        "class FashionImagesDataset(Dataset):\n",
        "    def __init__(self, transform=lambda id: id):\n",
        "        self.articles = pd.read_csv('/content/archive/articles.csv')\n",
        "        self.articles['img_path'] = self.articles['article_id'].map(lambda id: \"/content/archive/images/0\" + str(id)[0:2] + \"/0\" + str(id) + \".jpg\")\n",
        "        self.valid_idx = self.articles[self.articles.apply(lambda article: exists(article['img_path']), axis=1)]\n",
        "        print('valid and has image:', len(self.valid_idx), 'from:', len(self.articles))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_idx)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.valid_idx.iloc[idx]['img_path']\n",
        "        image = Image.open(img_path)\n",
        "        label = self.valid_idx.iloc[idx]['article_id']\n",
        "        image = self.transform(image)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej-lDKY8oSwy",
        "outputId": "7fb6cff5-7213-4adc-fc60-927af89e84e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RN50',\n",
              " 'RN101',\n",
              " 'RN50x4',\n",
              " 'RN50x16',\n",
              " 'RN50x64',\n",
              " 'ViT-B/32',\n",
              " 'ViT-B/16',\n",
              " 'ViT-L/14']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "model_name = 'ViT-B/32'\n",
        "# also ViT-L/14, etc.\n",
        "clip.available_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzA5ARbMTjUs",
        "outputId": "a22ee360-f14e-48f0-d8b5-f77ff7e0ec68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:01<00:00, 195MiB/s]\n"
          ]
        }
      ],
      "source": [
        "model, preprocess = clip.load(model_name, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr5tMoAPu_br"
      },
      "outputs": [],
      "source": [
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv4yU01t27gh",
        "outputId": "f8c962c4-4d31-45e5-bf9f-b55d2e001a65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid and has image: 105100 from: 105542\n"
          ]
        }
      ],
      "source": [
        "dataset = FashionImagesDataset(transform=preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gFqthT8uwAL"
      },
      "outputs": [],
      "source": [
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2yd9DQ3OzvI"
      },
      "outputs": [],
      "source": [
        "images, labels = next(iter(data_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBnOFKJHbVJq",
        "outputId": "e98ba432-e19a-4d2a-fa7f-7dde64c7e1ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(t.Size([64, 3, 224, 224]), t.Size([3, 224, 224]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "images.size(), images.chunk(batch_size)[1].squeeze().size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7NLfRnDmQ8y"
      },
      "source": [
        "# Get image embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOlIw9PAvDq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a91859d4-3436-4bdc-e46f-280faefa82f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1643/1643 [1:20:46<00:00,  2.95s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "image_features = {}\n",
        "with t.no_grad():\n",
        "    for images, labels in tqdm(data_loader):\n",
        "      features = model.encode_image(images.to(device))\n",
        "      for label, feature in zip(labels, features):\n",
        "        image_features[label.item()] = feature.to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VO51X0CIvFCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eefc2e9-9e09-4ac5-a79f-d917cdaeea41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "t.Size([512])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "image_features[111565003].size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQVGLfJDtn2M"
      },
      "source": [
        "# Save image embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl434UGWfahO"
      },
      "outputs": [],
      "source": [
        "file_name = '/content/fashion-recommendation-image-embeddings-clip-' + model_name.replace('/', '-') + '.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj8NalvkROkp"
      },
      "outputs": [],
      "source": [
        "t.save(image_features, file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tjr65BiVt_g",
        "outputId": "88de7b27-ec90-4617-d937-45ff49616d0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "105100"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "len(image_features.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKcXaC-oVoDQ",
        "outputId": "6760e137-546a-468c-d394-bb50d5c6ffcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 130M Apr  5 14:57 /content/fashion-recommendation-image-embeddings-clip-ViT-B-32.pt\n"
          ]
        }
      ],
      "source": [
        "!ls -lah $file_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKud3H9UorOw"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPAuaUMZt8r-",
        "outputId": "794ba02f-32f3-4a22-8ba8-1876f6722c3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file:///content/fashion-recommendation-image-embeddings-clip-ViT-B-32.pt [Content-Type=application/octet-stream]...\n",
            "|\n",
            "Operation completed over 1 objects/130.0 MiB.                                    \n"
          ]
        }
      ],
      "source": [
        "!gsutil cp $file_name gs://heii-public/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JXu3G_gd_tC"
      },
      "source": [
        "# Reload image embeddings (e.g.: if you restart colab, etc...)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = '/content/fashion-recommendation-image-embeddings-clip-' + model_name.replace('/', '-') + '.pt'"
      ],
      "metadata": {
        "id": "Fi4KCQs_RFiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB4rFFHD51E-"
      },
      "outputs": [],
      "source": [
        "remote_file = \"https://storage.googleapis.com/heii-public/\" + file_name.replace('/content/', '')\n",
        "remote_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT6eEfSGRHaI"
      },
      "outputs": [],
      "source": [
        "!wget $remote_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPQBHdusR6ZP"
      },
      "outputs": [],
      "source": [
        "image_features = t.load(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzSXKYloKtUt"
      },
      "source": [
        "# Get text embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "d3FRe7OTKPwK",
        "outputId": "0ec9d256-452c-48a9-c321-d0f98b61d135"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   article_id  product_code          prod_name  product_type_no  \\\n",
              "0   108775015        108775          Strap top              253   \n",
              "1   108775044        108775          Strap top              253   \n",
              "2   108775051        108775      Strap top (1)              253   \n",
              "3   110065001        110065  OP T-shirt (Idro)              306   \n",
              "4   110065002        110065  OP T-shirt (Idro)              306   \n",
              "\n",
              "  product_type_name  product_group_name  graphical_appearance_no  \\\n",
              "0          Vest top  Garment Upper body                  1010016   \n",
              "1          Vest top  Garment Upper body                  1010016   \n",
              "2          Vest top  Garment Upper body                  1010017   \n",
              "3               Bra           Underwear                  1010016   \n",
              "4               Bra           Underwear                  1010016   \n",
              "\n",
              "  graphical_appearance_name  colour_group_code colour_group_name  ...  \\\n",
              "0                     Solid                  9             Black  ...   \n",
              "1                     Solid                 10             White  ...   \n",
              "2                    Stripe                 11         Off White  ...   \n",
              "3                     Solid                  9             Black  ...   \n",
              "4                     Solid                 10             White  ...   \n",
              "\n",
              "   department_name index_code        index_name index_group_no  \\\n",
              "0     Jersey Basic          A        Ladieswear              1   \n",
              "1     Jersey Basic          A        Ladieswear              1   \n",
              "2     Jersey Basic          A        Ladieswear              1   \n",
              "3   Clean Lingerie          B  Lingeries/Tights              1   \n",
              "4   Clean Lingerie          B  Lingeries/Tights              1   \n",
              "\n",
              "   index_group_name section_no            section_name garment_group_no  \\\n",
              "0        Ladieswear         16  Womens Everyday Basics             1002   \n",
              "1        Ladieswear         16  Womens Everyday Basics             1002   \n",
              "2        Ladieswear         16  Womens Everyday Basics             1002   \n",
              "3        Ladieswear         61         Womens Lingerie             1017   \n",
              "4        Ladieswear         61         Womens Lingerie             1017   \n",
              "\n",
              "   garment_group_name                                        detail_desc  \n",
              "0        Jersey Basic            Jersey top with narrow shoulder straps.  \n",
              "1        Jersey Basic            Jersey top with narrow shoulder straps.  \n",
              "2        Jersey Basic            Jersey top with narrow shoulder straps.  \n",
              "3   Under-, Nightwear  Microfibre T-shirt bra with underwired, moulde...  \n",
              "4   Under-, Nightwear  Microfibre T-shirt bra with underwired, moulde...  \n",
              "\n",
              "[5 rows x 25 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3101af90-2a6f-4104-80cd-78fa7b609a1c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_id</th>\n",
              "      <th>product_code</th>\n",
              "      <th>prod_name</th>\n",
              "      <th>product_type_no</th>\n",
              "      <th>product_type_name</th>\n",
              "      <th>product_group_name</th>\n",
              "      <th>graphical_appearance_no</th>\n",
              "      <th>graphical_appearance_name</th>\n",
              "      <th>colour_group_code</th>\n",
              "      <th>colour_group_name</th>\n",
              "      <th>...</th>\n",
              "      <th>department_name</th>\n",
              "      <th>index_code</th>\n",
              "      <th>index_name</th>\n",
              "      <th>index_group_no</th>\n",
              "      <th>index_group_name</th>\n",
              "      <th>section_no</th>\n",
              "      <th>section_name</th>\n",
              "      <th>garment_group_no</th>\n",
              "      <th>garment_group_name</th>\n",
              "      <th>detail_desc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>108775015</td>\n",
              "      <td>108775</td>\n",
              "      <td>Strap top</td>\n",
              "      <td>253</td>\n",
              "      <td>Vest top</td>\n",
              "      <td>Garment Upper body</td>\n",
              "      <td>1010016</td>\n",
              "      <td>Solid</td>\n",
              "      <td>9</td>\n",
              "      <td>Black</td>\n",
              "      <td>...</td>\n",
              "      <td>Jersey Basic</td>\n",
              "      <td>A</td>\n",
              "      <td>Ladieswear</td>\n",
              "      <td>1</td>\n",
              "      <td>Ladieswear</td>\n",
              "      <td>16</td>\n",
              "      <td>Womens Everyday Basics</td>\n",
              "      <td>1002</td>\n",
              "      <td>Jersey Basic</td>\n",
              "      <td>Jersey top with narrow shoulder straps.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>108775044</td>\n",
              "      <td>108775</td>\n",
              "      <td>Strap top</td>\n",
              "      <td>253</td>\n",
              "      <td>Vest top</td>\n",
              "      <td>Garment Upper body</td>\n",
              "      <td>1010016</td>\n",
              "      <td>Solid</td>\n",
              "      <td>10</td>\n",
              "      <td>White</td>\n",
              "      <td>...</td>\n",
              "      <td>Jersey Basic</td>\n",
              "      <td>A</td>\n",
              "      <td>Ladieswear</td>\n",
              "      <td>1</td>\n",
              "      <td>Ladieswear</td>\n",
              "      <td>16</td>\n",
              "      <td>Womens Everyday Basics</td>\n",
              "      <td>1002</td>\n",
              "      <td>Jersey Basic</td>\n",
              "      <td>Jersey top with narrow shoulder straps.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>108775051</td>\n",
              "      <td>108775</td>\n",
              "      <td>Strap top (1)</td>\n",
              "      <td>253</td>\n",
              "      <td>Vest top</td>\n",
              "      <td>Garment Upper body</td>\n",
              "      <td>1010017</td>\n",
              "      <td>Stripe</td>\n",
              "      <td>11</td>\n",
              "      <td>Off White</td>\n",
              "      <td>...</td>\n",
              "      <td>Jersey Basic</td>\n",
              "      <td>A</td>\n",
              "      <td>Ladieswear</td>\n",
              "      <td>1</td>\n",
              "      <td>Ladieswear</td>\n",
              "      <td>16</td>\n",
              "      <td>Womens Everyday Basics</td>\n",
              "      <td>1002</td>\n",
              "      <td>Jersey Basic</td>\n",
              "      <td>Jersey top with narrow shoulder straps.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>110065001</td>\n",
              "      <td>110065</td>\n",
              "      <td>OP T-shirt (Idro)</td>\n",
              "      <td>306</td>\n",
              "      <td>Bra</td>\n",
              "      <td>Underwear</td>\n",
              "      <td>1010016</td>\n",
              "      <td>Solid</td>\n",
              "      <td>9</td>\n",
              "      <td>Black</td>\n",
              "      <td>...</td>\n",
              "      <td>Clean Lingerie</td>\n",
              "      <td>B</td>\n",
              "      <td>Lingeries/Tights</td>\n",
              "      <td>1</td>\n",
              "      <td>Ladieswear</td>\n",
              "      <td>61</td>\n",
              "      <td>Womens Lingerie</td>\n",
              "      <td>1017</td>\n",
              "      <td>Under-, Nightwear</td>\n",
              "      <td>Microfibre T-shirt bra with underwired, moulde...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>110065002</td>\n",
              "      <td>110065</td>\n",
              "      <td>OP T-shirt (Idro)</td>\n",
              "      <td>306</td>\n",
              "      <td>Bra</td>\n",
              "      <td>Underwear</td>\n",
              "      <td>1010016</td>\n",
              "      <td>Solid</td>\n",
              "      <td>10</td>\n",
              "      <td>White</td>\n",
              "      <td>...</td>\n",
              "      <td>Clean Lingerie</td>\n",
              "      <td>B</td>\n",
              "      <td>Lingeries/Tights</td>\n",
              "      <td>1</td>\n",
              "      <td>Ladieswear</td>\n",
              "      <td>61</td>\n",
              "      <td>Womens Lingerie</td>\n",
              "      <td>1017</td>\n",
              "      <td>Under-, Nightwear</td>\n",
              "      <td>Microfibre T-shirt bra with underwired, moulde...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 25 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3101af90-2a6f-4104-80cd-78fa7b609a1c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3101af90-2a6f-4104-80cd-78fa7b609a1c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3101af90-2a6f-4104-80cd-78fa7b609a1c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "articles = pd.read_csv('/content/archive/articles.csv')\n",
        "articles.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAzHLLetwZi7"
      },
      "outputs": [],
      "source": [
        "keys = ['derived_name', 'derived_look', 'derived_category', 'prod_name', 'product_type_name', 'product_group_name', 'graphical_appearance_name', 'colour_group_name', 'perceived_colour_value_name', 'department_name', 'index_name', 'index_group_name', 'section_name', 'garment_group_name', 'detail_desc']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjVdt3NxCw2Y",
        "outputId": "1b95170e-fd7c-49af-ddc3-7395f60532a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example derived values:\n",
            "This is a Strap top Vest top\n",
            "It has a Solid Dark Black color\n",
            "Ladieswear / Ladieswear / Womens Everyday Basics / Jersey Basic / Jersey Basic / Garment Upper body / Vest top / Solid / Black\n",
            "This is a Strap top Vest top\n",
            "It has a Solid Light White color\n",
            "Ladieswear / Ladieswear / Womens Everyday Basics / Jersey Basic / Jersey Basic / Garment Upper body / Vest top / Solid / White\n",
            "This is a Strap top (1) Vest top\n",
            "It has a Stripe Dusty Light Off White color\n",
            "Ladieswear / Ladieswear / Womens Everyday Basics / Jersey Basic / Jersey Basic / Garment Upper body / Vest top / Stripe / Off White\n"
          ]
        }
      ],
      "source": [
        "articles = pd.read_csv('/content/archive/articles.csv')\n",
        "articles['derived_name'] = articles.apply(lambda row: ' '.join(['This is a', row['prod_name'], row['product_type_name']]), axis=1)\n",
        "articles['derived_look'] = articles.apply(lambda row: ' '.join(['It has a', row['graphical_appearance_name'], row['perceived_colour_value_name'], row['colour_group_name'], 'color']), axis=1)\n",
        "articles['derived_category'] = articles.apply(lambda row: ' / '.join([row['index_group_name'], row['index_name'], row['section_name'], row['department_name'], row['garment_group_name'], row['product_group_name'], row['product_type_name'], row['graphical_appearance_name'], row['colour_group_name']]), axis=1)\n",
        "print('Example derived values:')\n",
        "for i in range(3):\n",
        "    print(articles.iloc[i]['derived_name'])\n",
        "    print(articles.iloc[i]['derived_look'])\n",
        "    print(articles.iloc[i]['derived_category'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLkSajvDCJ_l"
      },
      "outputs": [],
      "source": [
        "class FashionTextDataset(Dataset):\n",
        "    def __init__(self, key, articles):\n",
        "        self.key = key\n",
        "        self.articles = articles\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.articles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.articles.iloc[idx]['article_id']\n",
        "        # tokenize already pads text\n",
        "        tokens = clip.tokenize(str.strip(str(self.articles.iloc[idx][self.key])), 77, True).squeeze().to(device)\n",
        "        return tokens, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fhyQ-PBd38P"
      },
      "outputs": [],
      "source": [
        "text_dataset = FashionTextDataset(key=keys[0], articles=articles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CGUrTyne8eA"
      },
      "outputs": [],
      "source": [
        "text_data_loader = DataLoader(text_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFhL4WW4fCAB"
      },
      "outputs": [],
      "source": [
        "tokens, labels = next(iter(text_data_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gwb8oRGlntPt",
        "outputId": "7bb91574-9935-45d6-b74f-c93f6b6dfbaa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 15, t.Size([64, 77]))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "batch_size, len(keys), tokens.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "R-IkrZo3t7sX",
        "outputId": "c0461d08-626f-43ce-85a8-d1a53021c4ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|startoftext|>this is a strap top vest top <|endoftext|>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "tokenizer = clip.simple_tokenizer.SimpleTokenizer()\n",
        "tokenizer.decode(tokens[1].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUA3rK_rDzHQ",
        "outputId": "5766a5c1-0723-4f0d-87e2-81d3ec7ca9e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  derived_name text fields left: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:22<00:00,  8.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  derived_look text fields left: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:22<00:00,  8.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  derived_category text fields left: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:30<00:00,  7.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  prod_name text fields left: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:19<00:00,  8.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  product_type_name text fields left: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:17<00:00,  8.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  product_group_name text fields left: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:18<00:00,  8.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  graphical_appearance_name text fields left: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:17<00:00,  8.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  colour_group_name text fields left: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:17<00:00,  8.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  perceived_colour_value_name text fields left: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:16<00:00,  8.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  department_name text fields left: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:17<00:00,  8.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  index_name text fields left: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:17<00:00,  8.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  index_group_name text fields left: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:16<00:00,  8.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  section_name text fields left: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:17<00:00,  8.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  garment_group_name text fields left: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:17<00:00,  8.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  detail_desc text fields left: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:31<00:00,  7.82it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "text_features = {}\n",
        "with t.no_grad():\n",
        "  for i, key in enumerate(keys):\n",
        "      print('getting embeddings for ', key, 'text fields left:', len(keys)-i)\n",
        "      text_dataset_ = FashionTextDataset(key=key, articles=articles)\n",
        "      text_data_loader_ = DataLoader(text_dataset_, batch_size=64, shuffle=False)\n",
        "      for tokens, labels in tqdm(text_data_loader_):\n",
        "          features = model.encode_text(tokens)\n",
        "          for label, feature in zip(labels, features):\n",
        "              text_features.setdefault(label.item(), {})[key] = feature.to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjjHVDoyyT1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c258a6-9bdd-4eca-912d-36295a576b2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "t.Size([512])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "text_features[116379047]['derived_name'].size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qcj1I5gWzERX"
      },
      "source": [
        "# Save text embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xA0S0VzHGAG"
      },
      "source": [
        "should match len(articles):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtAvwcgtzRGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cba38a37-cbe6-4c36-dd4e-df687afa3e17"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105542, 105542, True, True)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "len(articles), len(text_features.keys()), len(articles) == len(text_features.keys()), all([512 == len(text_features[108775015].get(key, {})) for key in keys])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js4mog9QzEAf"
      },
      "outputs": [],
      "source": [
        "text_file_name = '/content/fashion-recommendation-text-embeddings-clip-' + model_name.replace('/', '-') + '.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRQ9knT-zLir"
      },
      "outputs": [],
      "source": [
        "t.save(text_features, text_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE55_L4DzW5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c4c7902-1252-4653-9639-f438d3e62c4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file:///content/fashion-recommendation-text-embeddings-clip-ViT-B-32.pt [Content-Type=application/octet-stream]...\n",
            "/ [0 files][    0.0 B/  1.9 GiB]                                                \r==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "-\n",
            "Operation completed over 1 objects/1.9 GiB.                                      \n"
          ]
        }
      ],
      "source": [
        "!gsutil cp $text_file_name gs://heii-public/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BPuES-KzyWC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0094b3da-97c9-4627-ae65-8888d93c2254"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://storage.googleapis.com/heii-public/fashion-recommendation-text-embeddings-clip-ViT-B-32.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "remote_text_file = \"https://storage.googleapis.com/heii-public/\" + text_file_name.replace('/content/', '')\n",
        "remote_text_file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reload text embeddings"
      ],
      "metadata": {
        "id": "XuJtf_mdYKsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file_name = '/content/fashion-recommendation-text-embeddings-clip-' + model_name.replace('/', '-') + '.pt'"
      ],
      "metadata": {
        "id": "HRba7tpqYg6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remote_text_file = \"https://storage.googleapis.com/heii-public/\" + text_file_name.replace('/content/', '')"
      ],
      "metadata": {
        "id": "RTxHWAqfYQ6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget $remote_text_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hdr4KJ7VYQjE",
        "outputId": "5b71d8bc-0435-48e4-9202-eb995387fde7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-05 11:33:10--  https://storage.googleapis.com/heii-public/fashion-recommendation-text-embeddings-clip-ViT-B-32.pt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.5.128, 74.125.133.128, 108.177.15.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.5.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 480994078 (459M) [application/octet-stream]\n",
            "Saving to: ‘fashion-recommendation-text-embeddings-clip-ViT-B-32.pt’\n",
            "\n",
            "fashion-recommendat 100%[===================>] 458.71M  95.1MB/s    in 5.2s    \n",
            "\n",
            "2022-04-05 11:33:16 (88.5 MB/s) - ‘fashion-recommendation-text-embeddings-clip-ViT-B-32.pt’ saved [480994078/480994078]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_features = t.load(text_file_name)"
      ],
      "metadata": {
        "id": "Fo7YZMq5ZOTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict customer - product interaction:\n",
        "prerequisites: run setup"
      ],
      "metadata": {
        "id": "LgAiDyoiZYiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as t\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.data import HeteroData, InMemoryDataset, download_url\n",
        "from torch_geometric.utils import negative_sampling\n",
        "\n",
        "\n",
        "class HMDataset(InMemoryDataset):\n",
        "    image_embeddings_url = \"https://storage.googleapis.com/heii-public/fashion-recommendation-image-embeddings-clip-ViT-B-32.pt\"\n",
        "    text_embeddings_url = \"https://storage.googleapis.com/heii-public/fashion-recommendation-text-embeddings-clip-ViT-B-32.pt\"\n",
        "    raw_dir = \"/content\"\n",
        "    processed_dir = \"/content\"\n",
        "\n",
        "    def __init__(self, root, transform=None, pre_transform=None):\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = t.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return [\n",
        "            \"fashion-recommendation-image-embeddings-clip-ViT-B-32.pt\",\n",
        "            \"fashion-recommendation-text-embeddings-clip-ViT-B-32.pt\",\n",
        "            \"archive/articles.csv\",\n",
        "            \"archive/customers.csv\",\n",
        "            \"archive/transactions_train.csv\",\n",
        "        ]\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return f\"hm_graph.pt\"\n",
        "\n",
        "    def download(self):\n",
        "        download_url(self.image_embeddings_url, self.raw_dir)\n",
        "        download_url(self.text_embeddings_url, self.raw_dir)\n",
        "\n",
        "    def process(self):\n",
        "        self.articles = pd.read_csv(self.raw_paths[2], index_col=\"article_id\")\n",
        "        self.customers = pd.read_csv(self.raw_paths[3], index_col=\"customer_id\").fillna(\n",
        "            0.0\n",
        "        )\n",
        "        self.transactions = pd.read_csv(self.raw_paths[4])\n",
        "\n",
        "        data = HeteroData()\n",
        "        \n",
        "        # create node edges\n",
        "        t = self.transactions.to_dict()\n",
        "        customers_id_ix = {v: k for k, v in enumerate(self.customers.index.unique())}\n",
        "        # customers_ix_id = {k: v for k, v in enumerate(self.customers.index.unique())}\n",
        "        articles_id_ix = {v: k for k, v in enumerate(self.articles.index.unique())}\n",
        "        # articles_ix_id = {k: v for k, v in enumerate(self.articles.index.unique())}\n",
        "        src = [customers_id_ix[t[\"customer_id\"][i]] for i in t[\"customer_id\"]]\n",
        "        dst = [articles_id_ix[t[\"article_id\"][i]] for i in t[\"article_id\"]]\n",
        "        data[\"customer\", \"buys\", \"article\"].edge_index = t.tensor([src, dst]).long()\n",
        "        \n",
        "        # avoid out of memory on colab\n",
        "        del t\n",
        "        del customers_id_ix\n",
        "        del articles_id_ix\n",
        "\n",
        "        # encode customers\n",
        "        le = preprocessing.LabelEncoder()\n",
        "        self.customers[\"postal_code\"] = le.fit_transform(self.customers[\"postal_code\"])\n",
        "        self.customers.loc[\n",
        "            self.customers[\"fashion_news_frequency\"] == \"None\", \"fashion_news_frequency\"\n",
        "        ] = 0.0\n",
        "        self.customers.loc[\n",
        "            self.customers[\"fashion_news_frequency\"] == \"NONE\", \"fashion_news_frequency\"\n",
        "        ] = 0.0\n",
        "        customer_features = self.customers[\n",
        "            [\n",
        "                \"postal_code\",\n",
        "                \"age\",\n",
        "                \"fashion_news_frequency\",\n",
        "                \"FN\",\n",
        "                \"Active\",\n",
        "                \"club_member_status\",\n",
        "            ]\n",
        "        ]\n",
        "        customer_features = pd.get_dummies(\n",
        "            customer_features,\n",
        "            columns=[\"age\", \"fashion_news_frequency\", \"club_member_status\"],\n",
        "        )\n",
        "        customer_features = t.from_numpy(customer_features.to_numpy())\n",
        "\n",
        "        # encode articles\n",
        "        self.article_image_embeddings = t.load(self.raw_paths[0])\n",
        "        self.article_text_embeddings = t.load(self.raw_paths[1])\n",
        "        self.articles = self.articles.merge(\n",
        "            self.transactions.groupby(\"article_id\")[\"price\"].mean(),\n",
        "            on=\"article_id\",\n",
        "            how=\"outer\",\n",
        "        ).fillna(0.0)\n",
        "        # self.articles[\"price_bin\"] = pd.qcut(self.articles[\"price\"], 100, labels=False)\n",
        "        self.articles[\"product_type_no\"] = self.articles[\"product_type_no\"].astype(str)\n",
        "        product_type_no_le = preprocessing.LabelEncoder()\n",
        "        self.articles[\"product_type_no\"] = product_type_no_le.fit_transform(\n",
        "            self.articles[\"product_type_no\"]\n",
        "        )\n",
        "        self.articles[\"graphical_appearance_no\"] = self.articles[\n",
        "            \"graphical_appearance_no\"\n",
        "        ].astype(str)\n",
        "        graphical_appearance_no_le = preprocessing.LabelEncoder()\n",
        "        self.articles[\n",
        "            \"graphical_appearance_no\"\n",
        "        ] = graphical_appearance_no_le.fit_transform(\n",
        "            self.articles[\"graphical_appearance_no\"]\n",
        "        )\n",
        "        article_features = self.articles[\n",
        "            [\"product_type_no\", \"graphical_appearance_no\", \"price\"]\n",
        "        ]\n",
        "        # article_features = pd.get_dummies(\n",
        "        #     article_features,\n",
        "        #     columns=[\"price_bin\"],\n",
        "        # )\n",
        "        article_features = t.from_numpy(article_features.to_numpy())\n",
        "        article_features = t.cat(\n",
        "            (\n",
        "                article_features,\n",
        "                t.stack(\n",
        "                    self.articles.apply(\n",
        "                        lambda article: self.article_image_embeddings.get(\n",
        "                            int(article.name), t.zeros(512)\n",
        "                        ),\n",
        "                        axis=1,\n",
        "                    ).tolist()\n",
        "                ),\n",
        "            ),\n",
        "            1,\n",
        "        )\n",
        "        for key in [\"derived_name\", \"derived_look\", \"derived_category\"]:\n",
        "            article_features = t.cat(\n",
        "                (\n",
        "                    article_features,\n",
        "                    t.stack(\n",
        "                        self.articles.apply(\n",
        "                            lambda article: self.article_text_embeddings[\n",
        "                                int(article.name)\n",
        "                            ].get(key, t.zeros(512)),\n",
        "                            axis=1,\n",
        "                        ).tolist()\n",
        "                    ),\n",
        "                ),\n",
        "                1,\n",
        "            )\n",
        "\n",
        "        # create nodes\n",
        "        data[\"article\"].x = article_features.float()\n",
        "        data[\"customer\"].x = customer_features.float()\n",
        "\n",
        "        # transform?\n",
        "        if self.pre_transform is not None:\n",
        "            data = self.pre_transform(data)\n",
        "\n",
        "        # PyTorch tensor functionality:\n",
        "        # data = data.pin_memory()\n",
        "        # data = data.to('cuda:0', non_blocking=True)\n",
        "        t.save(self.collate([data]), self.processed_paths[0])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset = HMDataset(\"/content\")"
      ],
      "metadata": {
        "id": "LbKL8eVIJs-b"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "import torch as t\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.loader import LinkNeighborLoader\n",
        "import torch_geometric.transforms as T\n",
        "# from hm_dataset import HMDataset\n",
        "from torch_geometric.nn import SAGEConv, to_hetero\n",
        "\n",
        "\n",
        "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "t.cuda.empty_cache()\n",
        "\n",
        "dataset = HMDataset(\"/content\")\n",
        "data = dataset[0] # .to(device)\n",
        "data[\"article\"].x = data[\"article\"].x.float()\n",
        "data[\"customer\"].x = data[\"customer\"].x.float()\n",
        "data[(\"customer\", \"buys\", \"article\")].edge_index = data[\n",
        "    (\"customer\", \"buys\", \"article\")\n",
        "].edge_index.long()\n",
        "\n",
        "# Add a reverse ('article', 'rev_buys', 'customer') relation for message passing:\n",
        "data = T.ToUndirected()(data).to('cpu')\n",
        "\n",
        "# Perform a link-level split into training, validation, and test edges:\n",
        "train_data, val_data, test_data = T.RandomLinkSplit(\n",
        "    num_val=0.1,\n",
        "    num_test=0.1,\n",
        "    neg_sampling_ratio=0.5,\n",
        "    add_negative_train_samples=True,\n",
        "    edge_types=[(\"customer\", \"buys\", \"article\")],\n",
        "    rev_edge_types=[(\"article\", \"rev_buys\", \"customer\")],\n",
        "    is_undirected=True,\n",
        ")(data)\n",
        "# when neg_sampling_ratio > 0 and add_negative_train_samples=True only then you will have negative edges\n",
        "\n",
        "def create_loader(d):\n",
        "    return LinkNeighborLoader(\n",
        "      d,\n",
        "      num_neighbors=[64] * 2,\n",
        "      batch_size=12,\n",
        "      edge_label_index=((\"customer\", \"buys\", \"article\"), d[(\"customer\", \"buys\", \"article\")].edge_label_index),\n",
        "      edge_label=d[(\"customer\", \"buys\", \"article\")].edge_label,\n",
        "      directed=False,\n",
        "      replace=False,\n",
        "      shuffle=True,\n",
        "      pin_memory=True,\n",
        "      num_workers=1\n",
        "    )\n",
        "train_loader = create_loader(train_data)\n",
        "val_loader = create_loader(val_data)\n",
        "test_loader = create_loader(test_data)\n",
        "\n",
        "\n",
        "class GNNEncoder(t.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
        "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EdgeDecoder(t.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.lin1 = Linear(2 * hidden_channels, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, 1)\n",
        "\n",
        "    def forward(self, z_dict, edge_label_index):\n",
        "        row, col = edge_label_index\n",
        "        z = t.cat([z_dict[\"customer\"][row], z_dict[\"article\"][col]], dim=-1)\n",
        "\n",
        "        z = self.lin1(z).relu()\n",
        "        z = self.lin2(z)\n",
        "        return z.view(-1)\n",
        "\n",
        "\n",
        "class Model(t.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
        "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr=\"sum\")\n",
        "        self.decoder = EdgeDecoder(hidden_channels)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
        "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
        "        return self.decoder(z_dict, edge_label_index)\n",
        "\n",
        "\n",
        "model = Model(hidden_channels=32).to(device)\n"
      ],
      "metadata": {
        "id": "zQXjTlyJiG8Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_state_dict(t.load(f\"/content/link_pred_0.pt\"))"
      ],
      "metadata": {
        "id": "BI4Wk2RS9Vqb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024"
      ],
      "metadata": {
        "id": "9BL-eZ8Q9r5B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Due to lazy initialization, we need to run one model step so the number\n",
        "# of parameters can be inferred:\n",
        "with t.no_grad():\n",
        "    batch_ = next(iter(train_loader)).to(device, non_blocking=True)\n",
        "    model.encoder(batch_.x_dict, batch_.edge_index_dict)\n",
        "    del batch_\n",
        "\n",
        "optimizer = t.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "def train(train_data):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(\n",
        "        train_data.x_dict,\n",
        "        train_data.edge_index_dict,\n",
        "        train_data[\"customer\", \"article\"].edge_label_index,\n",
        "    )\n",
        "    target = train_data[\"customer\", \"article\"].edge_label\n",
        "    loss = F.mse_loss(pred, target)\n",
        "    # loss = F.binary_cross_entropy_with_logits(pred, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "@t.no_grad()\n",
        "def test(data):\n",
        "    model.eval()\n",
        "    pred = model(\n",
        "        data.x_dict,\n",
        "        data.edge_index_dict,\n",
        "        data[\"customer\", \"article\"].edge_label_index,\n",
        "    )\n",
        "    pred = pred.clamp(min=0, max=1)\n",
        "    target = data[\"customer\", \"article\"].edge_label\n",
        "    loss = F.mse_loss(pred, target).sqrt()\n",
        "    # loss = t.nn.BCEWithLogitsLoss()(pred, target)\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "t.cuda.reset_peak_memory_stats()\n",
        "num_epochs = 301\n",
        "it = 0\n",
        "for epoch in range(1, num_epochs):\n",
        "    acc_loss = 0\n",
        "    train_steps, val_steps, test_steps = 200, 200, 200\n",
        "    prog = tqdm(zip(range(train_steps), train_loader), total=train_steps)\n",
        "    for i, batch in prog:\n",
        "        batch = batch.to(device, non_blocking=True)\n",
        "        loss = train(batch)\n",
        "        acc_loss += loss\n",
        "        it += 1\n",
        "        train_rmse = test(batch)\n",
        "        # cuda_mem = t.cuda.memory_stats()\n",
        "        # cuda_reserved = t.cuda.max_memory_reserved()\n",
        "        prog.set_description(f\"loss: {loss:.4f}\")\n",
        "    train_loss = acc_loss / it\n",
        "    for i, batch in tqdm(zip(range(train_steps), val_loader), total=val_steps):\n",
        "        val_rmse = test(batch.to(device, non_blocking=True))\n",
        "    for i, batch in tqdm(zip(range(train_steps), test_loader), total=test_steps):\n",
        "        test_rmse = test(batch.to(device, non_blocking=True))\n",
        "    if epoch % 30 == 0:\n",
        "        t.save(model.state_dict(), f\"/content/link_pred_{epoch:03d}.pt\")\n",
        "    print(\n",
        "        f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, AccLoss: {acc_loss:.4f}, Train: {train_loss:.4f}, TrainRMSE: {train_rmse:.4f}\"\n",
        "        f\"Val: {val_rmse:.4f}, Test: {test_rmse:.4f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfs_QnnTqD8J",
        "outputId": "c44e721b-59f4-49ec-eb35-14e4239ecf7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1112.6575: 100%|██████████| 200/200 [02:52<00:00,  1.16it/s]\n",
            "100%|██████████| 200/200 [02:48<00:00,  1.18it/s]\n",
            "100%|██████████| 200/200 [03:14<00:00,  1.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 1112.6575, AccLoss: 23607856.5081, Train: 118039.2825, TrainRMSE: 0.5000Val: 0.2887, Test: 0.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.3976:  65%|██████▌   | 130/200 [01:52<00:57,  1.23it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "83WLWnuYlKjN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "LliSeZa7yto5",
        "X7NLfRnDmQ8y",
        "yQVGLfJDtn2M",
        "5JXu3G_gd_tC",
        "zzSXKYloKtUt",
        "Qcj1I5gWzERX",
        "XuJtf_mdYKsE"
      ],
      "machine_shape": "hm",
      "name": "kaggle-fashion-clip.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}