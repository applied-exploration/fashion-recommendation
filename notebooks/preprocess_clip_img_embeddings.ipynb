{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "running on google colab..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx9pUccqlBh-"
      },
      "source": [
        "# Todo\n",
        "- [x] load clip\n",
        "- [x] load images\n",
        "- [x] batch run inference on images\n",
        "- [x] figure out how to load images from zip\n",
        "- [x] save embeddings\n",
        "- [x] benchmark, gpu: 2h\n",
        "- [x] upload to gcp bucket\n",
        "- [ ] embed text as well\n",
        "- [ ] predict a baseline score for kaggle, on image+description cos similarity only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8gReixDle8S"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK9UnBnZmmmZ"
      },
      "source": [
        "Don't forget to upload your **kaggle.json** for authentication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfKoG--brGPC"
      },
      "source": [
        "1. get data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yxJAauDm86J",
        "outputId": "495eb08b-0a7b-4670-b366-22b4be33aa8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/kaggle.json'\n",
            "Downloading h-and-m-personalized-fashion-recommendations.zip to /content\n",
            "100% 28.7G/28.7G [07:47<00:00, 72.9MB/s]\n",
            "100% 28.7G/28.7G [07:47<00:00, 65.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "!KAGGLE_CONFIG_DIR=/content kaggle competitions download -c h-and-m-personalized-fashion-recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clse0OTT0mpo"
      },
      "source": [
        "2. mount zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVEaOacCyXUG",
        "outputId": "08b99f7a-0405-4212-abdf-08ba36d8bcb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libzip4\n",
            "The following NEW packages will be installed:\n",
            "  fuse-zip libzip4\n",
            "0 upgraded, 2 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 65.6 kB of archives.\n",
            "After this operation, 178 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libzip4 amd64 1.1.2-1.1 [37.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fuse-zip amd64 0.4.4-1 [27.9 kB]\n",
            "Fetched 65.6 kB in 1s (53.8 kB/s)\n",
            "Selecting previously unselected package libzip4:amd64.\n",
            "(Reading database ... 156210 files and directories currently installed.)\n",
            "Preparing to unpack .../libzip4_1.1.2-1.1_amd64.deb ...\n",
            "Unpacking libzip4:amd64 (1.1.2-1.1) ...\n",
            "Selecting previously unselected package fuse-zip.\n",
            "Preparing to unpack .../fuse-zip_0.4.4-1_amd64.deb ...\n",
            "Unpacking fuse-zip (0.4.4-1) ...\n",
            "Setting up libzip4:amd64 (1.1.2-1.1) ...\n",
            "Setting up fuse-zip (0.4.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y fuse-zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C719KyYAxXcE"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/archive\n",
        "!fuse-zip /content/h-and-m-personalized-fashion-recommendations.zip /content/archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_njuz7q4zhr4"
      },
      "outputs": [],
      "source": [
        "# to unmount\n",
        "# !fusermount -u /content/archive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDFoZR1MrIt_"
      },
      "source": [
        "3. get clip model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg9N13AEq8JR",
        "outputId": "3a37b143-5270-4922-d6b6-e1b0e7c1fb52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▏                         | 10 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 40 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 53 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.63.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-4w2a7qnz\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-4w2a7qnz\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.63.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.11.1+cu111)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369221 sha256=bf1c16e1b9ae199dde2d65035619136d5f6f31e2d127c0fd0b140a915fc3d853\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-riaua1wx/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LliSeZa7yto5"
      },
      "source": [
        "# Get embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "P3X3L0ZkrUn2"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import clip\n",
        "\n",
        "device = \"cuda\" if t.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Mo41qRdisFPK"
      },
      "outputs": [],
      "source": [
        "# TODO to use tpu\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# device = xm.xla_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BhJdGWOip0W_"
      },
      "outputs": [],
      "source": [
        "# example useage of clip ala. https://github.com/openai/CLIP\n",
        "# model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "# image = preprocess(Image.open(\"/content/images/010/0108775015.jpg\")).unsqueeze(0).to(device)\n",
        "# text = clip.tokenize([\"a dress\", \"a dog\", \"a cat\"]).to(device)\n",
        "\n",
        "# with t.no_grad():\n",
        "#     image_features = model.encode_image(image)\n",
        "#     text_features = model.encode_text(text)\n",
        "    \n",
        "#     logits_per_image, logits_per_text = model(image, text)\n",
        "#     probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "# print(\"Label probs:\", probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qEDpSHBc5W71"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from t.utils.data import Dataset, DataLoader\n",
        "from os.path import exists\n",
        "\n",
        "\n",
        "class FashionImagesDataset(Dataset):\n",
        "    def __init__(self, transform=lambda id: id):\n",
        "        self.articles = pd.read_csv('/content/archive/articles.csv')\n",
        "        self.articles['img_path'] = self.articles['article_id'].map(lambda id: \"/content/archive/images/0\" + str(id)[0:2] + \"/0\" + str(id) + \".jpg\")\n",
        "        self.valid_idx = self.articles[self.articles.apply(lambda article: exists(article['img_path']), axis=1)]\n",
        "        print('valid and has image:', len(self.valid_idx), 'from:', len(self.articles))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_idx)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.valid_idx.iloc[idx]['img_path']\n",
        "        image = Image.open(img_path)\n",
        "        label = self.valid_idx.iloc[idx]['article_id']\n",
        "        image = self.transform(image)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej-lDKY8oSwy",
        "outputId": "950ee80e-afde-42ba-cd5b-b337cfa387c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['RN50',\n",
              " 'RN101',\n",
              " 'RN50x4',\n",
              " 'RN50x16',\n",
              " 'RN50x64',\n",
              " 'ViT-B/32',\n",
              " 'ViT-B/16',\n",
              " 'ViT-L/14']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_name = 'ViT-B/32'\n",
        "# also ViT-L/14, etc.\n",
        "clip.available_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzA5ARbMTjUs",
        "outputId": "f3113f73-39c7-4ef0-f4c7-65d75a66b1bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:06<00:00, 54.4MiB/s]\n"
          ]
        }
      ],
      "source": [
        "model, preprocess = clip.load(model_name, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Sr5tMoAPu_br"
      },
      "outputs": [],
      "source": [
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv4yU01t27gh",
        "outputId": "89f0e256-5664-47e2-e935-3108a3c12b61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid and has image: 105100 from: 105542\n"
          ]
        }
      ],
      "source": [
        "dataset = FashionImagesDataset(transform=preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9gFqthT8uwAL"
      },
      "outputs": [],
      "source": [
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "T2yd9DQ3OzvI"
      },
      "outputs": [],
      "source": [
        "images, labels = next(iter(data_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBnOFKJHbVJq",
        "outputId": "840ff469-9da3-4335-84e4-2e1d24637930"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(t.Size([64, 3, 224, 224]), t.Size([3, 224, 224]))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "images.size(), images.chunk(batch_size)[1].squeeze().size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOlIw9PAvDq5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "image_features = {}\n",
        "with t.no_grad():\n",
        "    for images, labels in tqdm(data_loader):\n",
        "      features = model.encode_image(images.to(device))\n",
        "      for label, feature in zip(labels, features):\n",
        "        image_features[label.item()] = feature.to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO51X0CIvFCM",
        "outputId": "45df347b-d77e-44c9-d0b9-c8e274008bd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "t.Size([512])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_features[111565003].size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQVGLfJDtn2M"
      },
      "source": [
        "# Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nl434UGWfahO"
      },
      "outputs": [],
      "source": [
        "file_name = '/content/fashion-recommendation-image-embeddings-clip-' + model_name.replace('/', '-') + '.pt'\n",
        "t.save(image_features, file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tjr65BiVt_g",
        "outputId": "301c8dfb-db8b-4043-96fd-3e499fa4301d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "105088"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(image_features.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKcXaC-oVoDQ",
        "outputId": "761e2f60-4e6a-4b8a-a1c2-4042c4b9b23f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 130M Mar 28 10:52 /content/fashion-recommendation-image-embeddings-clip-ViT-B-32.pt\n"
          ]
        }
      ],
      "source": [
        "!ls -lah $file_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mKud3H9UorOw"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPAuaUMZt8r-",
        "outputId": "3892ca4d-2210-4548-dbf6-95295512572f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying file:///content/fashion-recommendation-image-embeddings-clip-ViT-B-32.pt [Content-Type=application/octet-stream]...\n",
            "/\n",
            "Operation completed over 1 objects/130.0 MiB.                                    \n"
          ]
        }
      ],
      "source": [
        "!gsutil cp $file_name gs://heii-public/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uB4rFFHD51E-",
        "outputId": "b951d978-dea2-4a4a-f596-08f0c0babffd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://storage.googleapis.com/fashion-recommendation-image-embeddings-clip-ViT-B-32.pt'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"https://storage.googleapis.com/\" + file_name.replace('/content/', '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzSXKYloKtUt"
      },
      "source": [
        "# Get text embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3FRe7OTKPwK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYGJKKWuPyjY"
      },
      "source": [
        "predict images based on text descriptions, check accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noSJ3W68P5js"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w69cLtmgKxxd"
      },
      "source": [
        "# Predict clothes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMbagtEjKz0w"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "b8gReixDle8S"
      ],
      "name": "kaggle-fashion-clip.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}